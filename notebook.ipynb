{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74ade2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Twitter captions data\n",
    "labels = pd.read_excel(\"Captions.xlsx\")[[\"Caption\", \"image_id\"]]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2104ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a70bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "vgg = VGG16()\n",
    "vgg = Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract image features\n",
    "features = {}\n",
    "BASE_DIR = \"./Images/\"\n",
    "images = os.listdir(BASE_DIR)\n",
    "\n",
    "for img in tqdm(images):\n",
    "    try:\n",
    "        image_path = os.path.join(BASE_DIR, img)\n",
    "        image = Image.open(image_path).resize((224, 224))\n",
    "\n",
    "        image = img_to_array(image)\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = preprocess_input(image)\n",
    "\n",
    "        feature = vgg.predict(image, verbose=0)\n",
    "\n",
    "        image_idx = str(img).split(\".\")[0]\n",
    "        features[image_idx] = feature\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img}: {e}\")\n",
    "\n",
    "# Save features\n",
    "with open(\"image_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Twitter captions\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def remove_username(self, tweet):\n",
    "        return re.sub(r\"@(\\w+)\", \"\", tweet)\n",
    "    \n",
    "    def remove_punctuations(self, tweet):\n",
    "        puncs = [\".\", \",\", \"!\", \"?\", \":\", \";\", \"-\", \"_\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\", \"/\", \"\\\\\", \"|\", \"@\", \"$\", \"%\", \"^\", \"&\", \"*\", \"+\", \"=\", \"~\", \"`\", \"RT\"]\n",
    "        for punc in puncs:\n",
    "            tweet = tweet.replace(punc, \"\")\n",
    "        return tweet\n",
    "\n",
    "    def remove_stopwords(self, tweet):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tweets = [t for t in tweet.split(\" \") if t.lower() not in stop_words]\n",
    "        return \" \".join(filtered_tweets).strip()\n",
    "    \n",
    "    def lemmatize(self, tweet):\n",
    "        lemmatized = [self.lemmatizer.lemmatize(t) for t in tweet.split(\" \")]\n",
    "        return \" \".join(lemmatized)\n",
    "    \n",
    "    def stem_words(self, tweet):\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed = [stemmer.stem(t) for t in tweet.split(\" \")]\n",
    "        return \" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "preprocessor = Preprocessor()\n",
    "labels[\"Caption\"] = labels[\"Caption\"].fillna(\"\").astype(str)\n",
    "labels[\"Caption\"] = labels[\"Caption\"].apply(preprocessor.remove_username)\n",
    "labels[\"Caption\"] = labels[\"Caption\"].apply(preprocessor.remove_punctuations)\n",
    "labels[\"Caption\"] = labels[\"Caption\"].apply(preprocessor.remove_stopwords)\n",
    "labels[\"Caption\"] = labels[\"Caption\"].apply(preprocessor.lemmatize)\n",
    "\n",
    "# Add start/end tokens\n",
    "labels[\"Caption\"] = labels[\"Caption\"].apply(lambda x: \"<start> \" + x + \" <end>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1059058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image features\n",
    "with open(\"image_features.pkl\", \"rb\") as f:\n",
    "    image_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(labels[\"Caption\"].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for model\n",
    "def generate_data(features, labels, max_length, tokenizer, vocab_size):\n",
    "    image_features, i_sequence, o_sequence = [], [], []\n",
    "\n",
    "    for key in features.keys():\n",
    "        image_feature = features[key][0] #(4096,)\n",
    "\n",
    "        label = labels[labels[\"image_id\"] == f\"{key}.jpg\"][\"Caption\"].values[0]\n",
    "        label_split = tokenizer.texts_to_sequences([label])[0]\n",
    "        \n",
    "        for i in range(1, len(label_split)):\n",
    "            prev, next_word = label_split[:i], label_split[i]\n",
    "\n",
    "            in_seq = pad_sequences([prev], maxlen=max_length, padding='post')[0]\n",
    "            out_seq = to_categorical([next_word], num_classes=vocab_size)[0]\n",
    "\n",
    "            image_features.append(image_feature)\n",
    "            i_sequence.append(in_seq)\n",
    "            o_sequence.append(out_seq)\n",
    "            \n",
    "    return np.array(image_features), np.array(i_sequence), np.array(o_sequence)\n",
    "\n",
    "# Find max caption length\n",
    "max_length = max(len(caption.split()) for caption in labels[\"Caption\"])\n",
    "img_features, i_sequence, o_sequence = generate_data(image_features, labels, \n",
    "                                                    max_length=max_length, \n",
    "                                                    tokenizer=tokenizer, \n",
    "                                                    vocab_size=vocab_size)\n",
    "\n",
    "print(f\"Image features: {img_features.shape}, Input sequence: {i_sequence.shape}, Output sequence: {o_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "def caption_generator_model(vocab_size, max_length):\n",
    "    # Image feature branch\n",
    "    image_input = Input(shape=(4096,))\n",
    "    image_dropout = Dropout(0.5)(image_input)\n",
    "    image_dense = Dense(256, activation='relu')(image_dropout)\n",
    "\n",
    "    # Text branch\n",
    "    text_input = Input(shape=(max_length,))\n",
    "    text_embedding = Embedding(input_dim=vocab_size, output_dim=256)(text_input)\n",
    "    text_dropout = Dropout(0.5)(text_embedding)\n",
    "    text_lstm = LSTM(256)(text_dropout)\n",
    "\n",
    "    # Combined model\n",
    "    combined = add([image_dense, text_lstm])\n",
    "    dense = Dense(256, activation='relu')(combined)\n",
    "    output = Dense(vocab_size, activation='softmax')(dense)\n",
    "\n",
    "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "model = caption_generator_model(vocab_size=vocab_size, max_length=max_length)\n",
    "model.summary()\n",
    "\n",
    "# Train model\n",
    "model.fit([img_features, i_sequence], o_sequence, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(\"twitter_caption_model.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate captions\n",
    "def generate_caption(image_path, model, tokenizer, max_length):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).resize((224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "    \n",
    "    # Extract features\n",
    "    feature = vgg.predict(image, verbose=0)\n",
    "    \n",
    "    # Initialize caption\n",
    "    caption = \"<start>\"\n",
    "    \n",
    "    # Generate caption word by word\n",
    "    for _ in range(max_length):\n",
    "        # Tokenize current caption\n",
    "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        \n",
    "        # Predict next word\n",
    "        pred = model.predict([feature, sequence], verbose=0)\n",
    "        next_word_idx = np.argmax(pred[0])\n",
    "        \n",
    "        # Get word from index\n",
    "        next_word = tokenizer.index_word.get(next_word_idx, '')\n",
    "        \n",
    "        # Stop if we predict the end token\n",
    "        if next_word == 'end':\n",
    "            break\n",
    "            \n",
    "        # Append to caption\n",
    "        caption += \" \" + next_word\n",
    "    \n",
    "    # Remove start token\n",
    "    caption = caption.replace(\"<start>\", \"\").strip()\n",
    "    return caption\n",
    "\n",
    "# Test the model\n",
    "test_image = \"test.jpg\"  # Replace with your test image path\n",
    "generated_caption = generate_caption(test_image, model, tokenizer, max_length)\n",
    "print(\"Generated Caption:\", generated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
